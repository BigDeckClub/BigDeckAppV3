Nice catch — that comma-in-thousands issue is exactly what was sabotaging your price logic. You diagnosed it perfectly. Below I give a short explanation of what was missing, then a compact, battle-tested extraction + parsing utility you can drop into your Cheerio scraper, plus a few extra edge-case protections and debugging tips.

What you were missing (brief)

Your regex /\$([0-9]+\.[0-9]{2})/g only matches digits before a comma, so "$1,799.99" yields "1".

parseFloat("1") becomes 1.00, so premium items appear to be cheaper than normal cards and bypass your price <= 500 filter.

Solution: match comma-formatted numbers and strip commas before converting to float. Also be robust to non-breaking spaces and other odd characters.

Robust price-extraction & parsing utility (drop-in)
// utilities/priceUtils.js

/**
 * Find all price-like substrings in a text (handles commas, NBSP, etc.)
 * Returns an array of numbers (floats).
 */
function extractPricesFromText(text = '') {
  if (!text) return [];

  // Normalize NBSP and other whitespace, trim
  const normalized = text.replace(/\u00A0/g, ' ').trim();

  // Regex to capture numbers like:
  // $1,799.99   $1799.99   $0.99   $12.00
  // Explanation:
  //  \$               literal dollar sign
  //  ([0-9]{1,3}      1-3 digits
  //   (?:,[0-9]{3})*  zero or more groups of ,xxx (thousands)
  //   (?:\.[0-9]{2})? optional decimal .cc  (we usually expect it)
  //  )
  const re = /\$([0-9]{1,3}(?:,[0-9]{3})*(?:\.[0-9]{2})?)/g;

  const matches = [];
  let m;
  while ((m = re.exec(normalized)) !== null) {
    matches.push(m[1]); // raw captured like "1,799.99"
  }

  // Convert to numbers (strip commas)
  const numeric = matches
    .map(s => parseFloat(s.replace(/,/g, '')))
    .filter(n => !Number.isNaN(n));

  return numeric;
}

/**
 * Extract a single "best" numeric price from text.
 * strategy: if multiple prices present, return the smallest positive one.
 * returns null if none found.
 */
function getBestPriceFromText(text = '') {
  const prices = extractPricesFromText(text);
  if (!prices.length) return null;
  // filter sensible values (>= 0.01)
  const filtered = prices.filter(p => p >= 0.01 && Number.isFinite(p));
  if (!filtered.length) return null;
  return Math.min(...filtered);
}

/**
 * A convenience to safely parse a single price string like "$1,799.99"
 */
function parsePriceString(priceStr = '') {
  if (!priceStr) return null;
  // Remove currency, commas, NBSP, and any non-digit/dot characters, but prefer robust match above.
  const cleaned = priceStr.replace(/[\s\u00A0\$\,]/g, '').trim();
  const n = parseFloat(cleaned);
  return Number.isFinite(n) ? n : null;
}

module.exports = { extractPricesFromText, getBestPriceFromText, parsePriceString };

Example integration with Cheerio (products loop + logging)
const cheerio = require('cheerio');
const { extractPricesFromText, getBestPriceFromText } = require('./utilities/priceUtils');

function scrapeProducts(html, cardNameLower) {
  const $ = cheerio.load(html);

  const productEls = $('div[class*="product"], li[class*="product"], .catalog-item'); // your selectors
  const products = [];

  productEls.each((i, el) => {
    const el$ = $(el);
    const rawName = el$.find('a, .product-name, .item-title').first().text().trim();
    const name = rawName.toLowerCase();

    // Try direct price element(s) first
    const priceCandidates = el$.find('span.stylePrice, span.price, div.price').map((j, p) => $(p).text().trim()).get();

    // Also grab the whole product HTML for regex fallback
    const productHtml = el$.html();

    // Collect all numbers we can find in the element: from element text and fallback HTML.
    const numericPrices = [
      ...priceCandidates.map(t => getBestPriceFromText(t)).filter(x => x != null),
      getBestPriceFromText(productHtml) // fallback
    ].filter(x => x != null);

    // For debugging: show raw -> parsed
    console.log(`Product #${i + 1}`);
    console.log('  rawName:', rawName);
    console.log('  priceCandidates:', priceCandidates);
    console.log('  numericPrices (parsed):', numericPrices);

    if (name.includes(cardNameLower)) {
      // store the minimum price found for this product node
      const price = numericPrices.length ? Math.min(...numericPrices) : null;
      products.push({ name: rawName, price });
    } else {
      console.log(`  skipped (name mismatch)`);
    }
  });

  return products;
}

Extra edge-cases & suggestions

Price ranges (e.g. $1.50 - $2.00): the extractPricesFromText regex will capture both. getBestPriceFromText returns the smaller one. If you prefer the higher bound, change selection accordingly.

Non-breaking spaces / weird characters: we normalize \u00A0. There may be other unicode spaces; .replace(/\s+/g,' ') also helps.

Currency other than $: adjust regex if needed.

Thousands without commas: fine — we match both.

Malformed markup magically combining $1 and ,799.99 across nodes: since we run regex on the whole product HTML as a fallback, we should capture commas grouped correctly. If CK splits number across tags causing false matches, the robust selector + price element (span.stylePrice) approach is preferred.

Sanity filter: after parsing, re-apply isValidPrice = (p) => p >= 0.01 && p <= 500 (or whatever ceiling). That will remove premium variants after they are parsed correctly.

Always log original raw price strings together with parsed numeric values during debug runs so you can see what was present and how you parsed it.

Quick unit test examples (Node)
const { extractPricesFromText, parsePriceString } = require('./utilities/priceUtils');

console.assert(extractPricesFromText('$1,799.99')[0] === 1799.99, 'thousands comma');
console.assert(extractPricesFromText('$2.29')[0] === 2.29, 'simple');
console.assert(extractPricesFromText('Price: $1,099.99 and $0.99').length === 2, 'multiple');
console.assert(parsePriceString('$1,799.99') === 1799.99, 'parse helper');

Final checklist for you to apply

Replace your old regex /\$([0-9]+\.[0-9]{2})/g with the ones above (or use extractPricesFromText helper).

Strip commas before parseFloat (we do that).

Re-run your scraper and re-run the >= 0.01 && <= 500 sanity filter — premium variants will now be >500 and drop out.

Keep verbose logs for a few runs to confirm correct behavior, then reduce log level.